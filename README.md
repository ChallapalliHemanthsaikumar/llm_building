ğŸ—“ï¸ 30-Day Project Plan: Build a Reasoning LLM
Week 1: Neural Network & Language Basics
ğŸ‘‰ Focus: fundamentals of neural nets & word representation.
â€¢ Day 1: Implement perceptron (AND/OR/XOR).
â€¢ Day 2: Build a small feedforward NN on MNIST digits (only 1 hidden layer).
â€¢ Day 3: Implement backpropagation manually (tiny dataset).
â€¢ Day 4: Implement word embeddings (Word2Vec skip-gram with toy corpus).
â€¢ Day 5: Visualize embeddings with PCA/t-SNE.
â€¢ Day 6: Build an MLP for text classification (movie reviews tiny dataset).
â€¢ Day 7: Reflection & mini-report: compare embeddings vs one-hot.
Week 2: Sequence Models & Attention
ğŸ‘‰ Focus: sequence learning, RNN â†’ Transformer motivation.
â€¢ Day 8: Implement a vanilla RNN (char-level text generator).
â€¢ Day 9: Add LSTM, train on small text dataset (predict next char).
â€¢ Day 10: Compare RNN vs LSTM outputs on long sequences.
â€¢ Day 11: Implement self-attention (Q, K, V matrices) from scratch.
â€¢ Day 12: Visualize attention weights on a toy sentence.
â€¢ Day 13: Implement multi-head self-attention.
â€¢ Day 14: Mini reflection: why attention > RNNs?
Week 3: Transformers & Tiny GPT
ğŸ‘‰ Focus: build a decoder-only transformer.
â€¢ Day 15: Implement transformer block (attention + feedforward + layer norm).
â€¢ Day 16: Stack multiple transformer blocks â†’ mini encoder.
â€¢ Day 17: Add causal masking â†’ decoder-only transformer.
â€¢ Day 18: Train on a toy text corpus (fairy tales, Wikipedia snippets).
â€¢ Day 19: Generate text with sampling (top-k, nucleus sampling).
â€¢ Day 20: Implement positional embeddings & compare with/without them.
â€¢ Day 21: Reflection: How GPT is built from these blocks.
Week 4: Reasoning & Alignment
ğŸ‘‰ Focus: reasoning tasks + preference alignment.
â€¢ Day 22: Create toy reasoning dataset (math: 2+3=?, logical: â€œAâ†’B, Bâ†’Câ€).
â€¢ Day 23: Train GPT-mini on this reasoning dataset.
â€¢ Day 24: Add chain-of-thought fine-tuning (teach model to â€œthink step by stepâ€).
â€¢ Day 25: Evaluate on GSM8k-style toy problems.
â€¢ Day 26: Implement PPO (basic version) for reward tuning (reward = correct reasoning).
â€¢ Day 27: Try DPO with reasoning preferences (good vs bad chains).
â€¢ Day 28: Add tool-use module (model can call calculator).
â€¢ Day 29: Run evaluation: compare plain GPT-mini vs CoT vs PPO/DPO tuned.
â€¢ Day 30: Final report + presentation notebook (plots, samples, what worked, what didnâ€™t).
âœ… Deliverables by End of Month
â€¢ Mini GPT-like model (decoder-only, trained on toy reasoning data).
â€¢ Chain-of-thought reasoning fine-tuned model.
â€¢ PPO/DPO aligned version.
â€¢ A final notebook showing reasoning examples.